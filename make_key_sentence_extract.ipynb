{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keysentence_extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile keysentence_extract.py\n",
    "from konlpy.tag import Komoran\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import scipy as sp\n",
    "\n",
    "# Not_Key_sentence_RESUME 테이블에 insert\n",
    "def notkeysen_insert(ModelID, URI, RESUME_NO, SENTENCE, RESUME):\n",
    "\n",
    "    try:\n",
    "        conn = pymysql.connect(host=config.DATABASE_CONFIG['host'], port=config.DATABASE_CONFIG['port'],\n",
    "                               user=config.DATABASE_CONFIG['user'],\n",
    "                               passwd=config.DATABASE_CONFIG['password'],\n",
    "                               db=config.DATABASE_CONFIG['dbname'],\n",
    "                               charset=config.DATABASE_CONFIG['charset'])\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        SQL_Keyword_Insert = \"INSERT INTO `RESULT_PFA02_NOTKEYSEN` \\\n",
    "                        (`ModelID`, `URI`, `RESUME_NO`, `SENTENCE`, `RESUME`) \\\n",
    "                          VALUES ('%s', '%s', '%s', '%s', '%s')\" % (ModelID, URI, RESUME_NO, SENTENCE, RESUME)\n",
    "\n",
    "        curs.execute(SQL_Keyword_Insert)\n",
    "        curs.fetchall()\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        # print(\"URI : \", URI, \" / RESUME_NO : \", RESUME_NO, \" => [RESULT_PFA06_KEYWORD] 테이블 입력 완료!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print('RESULT_PFA02_NotKeySen TABLE Insert EXCEPTION ', str(e))\n",
    "        print(\"해당 URI의 자소서는 Insert할 수 없습니다. ==>\", URI, \"자소서 번호 ==> \", RESUME_NO)\n",
    "        pass\n",
    "\n",
    "\n",
    "def notkeysen_insert(ModelID, URI, RESUME_NO, SENTENCE, RESUME):\n",
    "\n",
    "    try:\n",
    "        conn = pymysql.connect(host=config.DATABASE_CONFIG['host'], port=config.DATABASE_CONFIG['port'],\n",
    "                               user=config.DATABASE_CONFIG['user'],\n",
    "                               passwd=config.DATABASE_CONFIG['password'],\n",
    "                               db=config.DATABASE_CONFIG['dbname'],\n",
    "                               charset=config.DATABASE_CONFIG['charset'])\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        SQL_Keyword_Insert = \"INSERT INTO `RESULT_PFA02_NOTKEYSEN` \\\n",
    "                        (`ModelID`, `URI`, `RESUME_NO`, `SENTENCE`, `RESUME`) \\\n",
    "                          VALUES ('%s', '%s', '%s', '%s', '%s')\" % (ModelID, URI, RESUME_NO, SENTENCE, RESUME)\n",
    "\n",
    "        curs.execute(SQL_Keyword_Insert)\n",
    "        curs.fetchall()\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        # print(\"URI : \", URI, \" / RESUME_NO : \", RESUME_NO, \" => [RESULT_PFA06_KEYWORD] 테이블 입력 완료!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print('RESULT_PFA02_NotKeySen TABLE Insert EXCEPTION ', str(e))\n",
    "        print(\"해당 URI의 자소서는 Insert할 수 없습니다. ==>\", URI, \"자소서 번호 ==> \", RESUME_NO)\n",
    "        pass\n",
    "\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "                 min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "                 df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "                       self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents\n",
    "\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    # check bias\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1, 1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1, 1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
    "               similarity=None, vocab_to_idx=None, verbose=False):\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x: x[1])]\n",
    "\n",
    "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
    "    if similarity == 'cosine':\n",
    "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    else:\n",
    "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    return x\n",
    "\n",
    "\n",
    "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, sent in enumerate(sents):\n",
    "        counter = Counter(tokenize(sent))\n",
    "        for token, count in counter.items():\n",
    "            j = vocab_to_idx.get(token, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    n_rows = len(sents)\n",
    "    n_cols = len(vocab_to_idx)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "\n",
    "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
    "    n_rows = x.shape[0]\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx + 1) * batch_size))\n",
    "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
    "        rows, cols = np.where(psim >= min_sim)\n",
    "        data = psim[rows, cols]\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e - b, n_rows)))\n",
    "        if verbose:\n",
    "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
    "    return mat\n",
    "\n",
    "\n",
    "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Boolean matrix\n",
    "    rows, cols = x.nonzero()\n",
    "    data = np.ones(rows.shape[0])\n",
    "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "    # Inverse sentence length\n",
    "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
    "    size[np.where(size <= min_length)] = 10000\n",
    "    size = np.log(size)\n",
    "\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "\n",
    "        # slicing\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx + 1) * batch_size))\n",
    "\n",
    "        # dot product\n",
    "        inner = z[b:e, :] * z.transpose()\n",
    "\n",
    "        # sentence len[i,j] = size[i] + size[j]\n",
    "        norm = size[b:e].reshape(-1, 1) + size.reshape(1, -1)\n",
    "        norm = norm ** (-1)\n",
    "        norm[np.where(norm == np.inf)] = 0\n",
    "\n",
    "        # normalize\n",
    "        sim = inner.multiply(norm).tocsr()\n",
    "        rows, cols = (sim >= min_sim).nonzero()\n",
    "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
    "\n",
    "        # append\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e - b, n_rows)))\n",
    "\n",
    "        if verbose:\n",
    "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
    "    if similarity == 'cosine':\n",
    "        similarity = cosine_sent_sim\n",
    "    elif callable(similarity):\n",
    "        similarity = similarity\n",
    "    else:\n",
    "        similarity = textrank_sent_sim\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    if verbose:\n",
    "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w: c for w, c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x: -x[1])]\n",
    "    vocab_to_idx = {vocab: idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "\n",
    "def tokenize_sents(sents, tokenize):\n",
    "    return [tokenize(sent) for sent in sents]\n",
    "\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "\n",
    "def komoran_tokenizer(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "komoran = Komoran()\n",
    "\n",
    "def keysentence_extract(RESUME):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        RESUME = re.sub(\"[^ㄱ-ㅣ가-힣|a-zA-Z|0-9|.]+\", \" \", RESUME) \n",
    "\n",
    "        RESUME_SENT = sent_tokenize(RESUME)\n",
    "\n",
    "        summarizer = KeysentenceSummarizer(tokenize=komoran_tokenizer, min_sim=0.0000001)\n",
    "        keysents = summarizer.summarize(RESUME_SENT, topk=100)\n",
    "\n",
    "\n",
    "        # 변수 선언\n",
    "        sent_idx_ordered = OrderedDict()  # textrank 계산이 완료된 자소서를 담는다.\n",
    "\n",
    "        NotKeySen_YN = \"N\"  # NoKeySen이 있는 자소서인지 표기한다.\n",
    "\n",
    "        recover_resume = \"\"  # 해당 문장에 태그를 붙여 자소서 원문을 만든다.\n",
    "        NotKeySen = \"\"  # 비중요 문장을 담는다.\n",
    "\n",
    "        # 계산 결과를 담는다.\n",
    "        for sent_idx, score, sent in keysents:\n",
    "            sent_idx_ordered[sent_idx] = [score, sent]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"중요문장이 추출되지 않았습니다.\"\n",
    "    \n",
    "    return sent_idx_ordered[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME = \"[영선반보의 자세로 우뚝 서다]  롯데정보통신은 지속적인 IT 혁신을 통해 롯데의 전 계열사와 유통, 서비스, 제조, 그리고 금융의 전 분야에서 최적의 서비스를 제공하고 성장해나가고 있습니다. 뿐만 아니라, 롯데 그룹의 한가운데에서 흔들리지 않는 나무처럼 든든한 버팀목이 되어주고 있습니다. 이러한 롯데정보통신에서 저의 역량을 120% 발휘해 더 큰 미래를 만들어 나가고자 지원하였습니다.   IT 산업은 그 어떤 산업보다 하루가 다르게 변화와 성장을 거듭하고 있으며 전 분야에서 IT 관련 지식 보유의 필요성이 대두하고 있습니다. 그래서 저는 경영학도이지만 IT라는 새로운 학문에 도전하며 융합적 인재로 성장하였습니다. 이러한 저의 역량으로 롯데정보통신과 함께 영선반보의 자세로 끊임없이 반 발자국 빠르게 도전하며 IT 산업 속에서 우뚝 서겠습니다.\"\n",
    "result = keysentence_extract(RESUME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4 on Python 3.6 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
